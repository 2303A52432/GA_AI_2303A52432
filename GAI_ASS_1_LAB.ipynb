{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOwsx+f3GtGvs300MhCeo0l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2303A52432/GA_AI_2303A52432/blob/main/GAI_ASS_1_LAB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A.YASHWANTH\n",
        "\n",
        "BATCH>>42\n",
        "\n",
        "2303A52432\n",
        "\n",
        "ASSIGNMENT-1"
      ],
      "metadata": {
        "id": "_hIoK3he2mZI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfScmpK_xq70",
        "outputId": "d40874b9-e15e-4f67-b702-7c321792bcae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error metrics from scratch:\n",
            "Mean Absolute Error (MAE): 0.4960000000000001\n",
            "Mean Squared Error (MSE): 0.2769200000000001\n",
            "Root Mean Squared Error (RMSE): 0.5262318880493656\n",
            "R-squared (R²): 0.9986154\n",
            "\n",
            "Error metrics from sklearn:\n",
            "Mean Absolute Error (MAE): 0.4960000000000001\n",
            "Mean Squared Error (MSE): 0.2769200000000001\n",
            "Root Mean Squared Error (RMSE): 0.5262318880493656\n",
            "R-squared (R²): 0.9986154\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Actual values and predicted values from the table\n",
        "YActual = np.array([20, 30, 40, 50, 60])\n",
        "YPred = np.array([20.530, 30.340, 40.250, 50.660, 60.7])\n",
        "\n",
        "# 1. Mean Absolute Error (MAE)\n",
        "MAE_scratch = np.mean(np.abs(YActual - YPred))\n",
        "\n",
        "# 2. Mean Squared Error (MSE)\n",
        "MSE_scratch = np.mean((YActual - YPred) ** 2)\n",
        "\n",
        "# 3. Root Mean Squared Error (RMSE)\n",
        "RMSE_scratch = np.sqrt(MSE_scratch)\n",
        "\n",
        "# 4. R-squared (R²)\n",
        "SS_total = np.sum((YActual - np.mean(YActual)) ** 2)\n",
        "SS_residual = np.sum((YActual - YPred) ** 2)\n",
        "R2_scratch = 1 - (SS_residual / SS_total)\n",
        "\n",
        "# Print the error metrics calculated from scratch\n",
        "print(\"Error metrics from scratch:\")\n",
        "print(f\"Mean Absolute Error (MAE): {MAE_scratch}\")\n",
        "print(f\"Mean Squared Error (MSE): {MSE_scratch}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {RMSE_scratch}\")\n",
        "print(f\"R-squared (R²): {R2_scratch}\")\n",
        "\n",
        "# 5. Compare with sklearn library functions\n",
        "MAE_sklearn = mean_absolute_error(YActual, YPred)\n",
        "MSE_sklearn = mean_squared_error(YActual, YPred)\n",
        "RMSE_sklearn = np.sqrt(MSE_sklearn)\n",
        "R2_sklearn = r2_score(YActual, YPred)\n",
        "\n",
        "# Print the error metrics from sklearn\n",
        "print(\"\\nError metrics from sklearn:\")\n",
        "print(f\"Mean Absolute Error (MAE): {MAE_sklearn}\")\n",
        "print(f\"Mean Squared Error (MSE): {MSE_sklearn}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {RMSE_sklearn}\")\n",
        "print(f\"R-squared (R²): {R2_sklearn}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# Actual values and predicted values from the table\n",
        "YActual = np.array([0, 1, 2, 0, 1, 0, 0, 1, 1, 2])\n",
        "YPred = np.array([0, 1, 0, 0, 0, 0, 1, 2, 2, 2])\n",
        "\n",
        "# 1. Accuracy\n",
        "accuracy_scratch = np.mean(YActual == YPred)\n",
        "\n",
        "# 2. Precision (for each class)\n",
        "# We calculate Precision for each class separately (0, 1, and 2)\n",
        "def precision(y_true, y_pred, class_label):\n",
        " TP = np.sum((y_true == class_label) & (y_pred == class_label))  # True positives\n",
        " FP = np.sum((y_true != class_label) & (y_pred == class_label))  # False positives\n",
        " return TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "\n",
        "precision_scratch = [precision(YActual, YPred, i) for i in np.unique(YActual)]\n",
        "\n",
        "# 3. Recall (for each class)\n",
        "def recall(y_true, y_pred, class_label):\n",
        " TP = np.sum((y_true == class_label) & (y_pred == class_label))  # True positives\n",
        " FN = np.sum((y_true == class_label) & (y_pred != class_label))  # False negatives\n",
        " return TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "\n",
        "recall_scratch = [recall(YActual, YPred, i) for i in np.unique(YActual)]\n",
        "\n",
        "# 4. F1-Score (for each class)\n",
        "def f1_score_class(precision, recall):\n",
        " return 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "f1_scratch = [f1_score_class(p, r) for p, r in zip(precision_scratch, recall_scratch)]\n",
        "\n",
        "# Print the metrics calculated from scratch\n",
        "print(\"Evaluation metrics from scratch:\")\n",
        "print(f\"Accuracy: {accuracy_scratch}\")\n",
        "print(f\"Precision for each class (0, 1, 2): {precision_scratch}\")\n",
        "print(f\"Recall for each class (0, 1, 2): {recall_scratch}\")\n",
        "print(f\"F1-Score for each class (0, 1, 2): {f1_scratch}\")\n",
        "\n",
        "# 5. Compare with sklearn library functions\n",
        "accuracy_sklearn = accuracy_score(YActual, YPred)\n",
        "precision_sklearn = precision_score(YActual, YPred, average=None, labels=np.unique(YActual))\n",
        "recall_sklearn = recall_score(YActual, YPred, average=None, labels=np.unique(YActual))\n",
        "f1_sklearn = f1_score(YActual, YPred, average=None, labels=np.unique(YActual))\n",
        "\n",
        "# Print the metrics from sklearn\n",
        "print(\"\\nEvaluation metrics from sklearn:\")\n",
        "print(f\"Accuracy: {accuracy_sklearn}\")\n",
        "print(f\"Precision for each class (0, 1, 2): {precision_sklearn}\")\n",
        "print(f\"Recall for each class (0, 1, 2): {recall_sklearn}\")\n",
        "print(f\"F1-Score for each class (0, 1, 2): {f1_sklearn}\")\n",
        "\n",
        "# 6. Confusion Matrix (optional but useful for classification problems)\n",
        "conf_matrix = confusion_matrix(YActual, YPred)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(conf_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8zYVQf90sf5",
        "outputId": "5426f938-3335-4d9f-9a34-8a80f554db9a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation metrics from scratch:\n",
            "Accuracy: 0.5\n",
            "Precision for each class (0, 1, 2): [0.6, 0.5, 0.3333333333333333]\n",
            "Recall for each class (0, 1, 2): [0.75, 0.25, 0.5]\n",
            "F1-Score for each class (0, 1, 2): [0.6666666666666665, 0.3333333333333333, 0.4]\n",
            "\n",
            "Evaluation metrics from sklearn:\n",
            "Accuracy: 0.5\n",
            "Precision for each class (0, 1, 2): [0.6        0.5        0.33333333]\n",
            "Recall for each class (0, 1, 2): [0.75 0.25 0.5 ]\n",
            "F1-Score for each class (0, 1, 2): [0.66666667 0.33333333 0.4       ]\n",
            "\n",
            "Confusion Matrix:\n",
            "[[3 1 0]\n",
            " [1 1 2]\n",
            " [1 0 1]]\n"
          ]
        }
      ]
    }
  ]
}