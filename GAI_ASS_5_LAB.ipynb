{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP+ZmFbLvDqC3lH6ezCKCN1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2303A52432/GA_AI_2303A52432/blob/main/GAI_ASS_5_LAB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o24QwgrAGU4w",
        "outputId": "3c3f0720-a396-494a-9e05-dbfe5b5260e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.25603874750047334\n",
            "Epoch 100, Loss: 0.0024200701256905134\n",
            "Epoch 200, Loss: 0.002370280726226115\n",
            "Epoch 300, Loss: 0.002318398772053936\n",
            "Epoch 400, Loss: 0.002264363106600211\n",
            "Epoch 500, Loss: 0.00220812140405195\n",
            "Epoch 600, Loss: 0.0021496330854220286\n",
            "Epoch 700, Loss: 0.0020888727335660576\n",
            "Epoch 800, Loss: 0.002025834021004182\n",
            "Epoch 900, Loss: 0.0019605341402121183\n",
            "Epoch 1000, Loss: 0.0018930186849013657\n",
            "Epoch 1100, Loss: 0.0018233668737676347\n",
            "Epoch 1200, Loss: 0.0017516969335769643\n",
            "Epoch 1300, Loss: 0.0016781713665897653\n",
            "Epoch 1400, Loss: 0.0016030017219082343\n",
            "Epoch 1500, Loss: 0.0015264523801557003\n",
            "Epoch 1600, Loss: 0.0014488427613526627\n",
            "Epoch 1700, Loss: 0.0013705472997168893\n",
            "Epoch 1800, Loss: 0.0012919925255369094\n",
            "Epoch 1900, Loss: 0.0012136506849339745\n",
            "Epoch 2000, Loss: 0.0011360295399557966\n",
            "Epoch 2100, Loss: 0.0010596583349629374\n",
            "Epoch 2200, Loss: 0.0009850703738204364\n",
            "Epoch 2300, Loss: 0.0009127831726340438\n",
            "Epoch 2400, Loss: 0.0008432776451619374\n",
            "Epoch 2500, Loss: 0.0007769781311156212\n",
            "Epoch 2600, Loss: 0.0007142351861269253\n",
            "Epoch 2700, Loss: 0.0006553128525315071\n",
            "Epoch 2800, Loss: 0.0006003816291436006\n",
            "Epoch 2900, Loss: 0.0005495176415385453\n",
            "Epoch 3000, Loss: 0.0005027077268494626\n",
            "Epoch 3100, Loss: 0.0004598594503957948\n",
            "Epoch 3200, Loss: 0.0004208145957570915\n",
            "Epoch 3300, Loss: 0.0003853644790839066\n",
            "Epoch 3400, Loss: 0.00035326552130862405\n",
            "Epoch 3500, Loss: 0.0003242537988662649\n",
            "Epoch 3600, Loss: 0.0002980576875942416\n",
            "Epoch 3700, Loss: 0.00027440812251951024\n",
            "Epoch 3800, Loss: 0.00025304634987124975\n",
            "Epoch 3900, Loss: 0.00023372931059190754\n",
            "Epoch 4000, Loss: 0.00021623295915240096\n",
            "Epoch 4100, Loss: 0.00020035389931857817\n",
            "Epoch 4200, Loss: 0.0001859097308436842\n",
            "Epoch 4300, Loss: 0.00017273847071201488\n",
            "Epoch 4400, Loss: 0.00016069735931751892\n",
            "Epoch 4500, Loss: 0.00014966130059523253\n",
            "Epoch 4600, Loss: 0.00013952112527546427\n",
            "Epoch 4700, Loss: 0.00013018181347760062\n",
            "Epoch 4800, Loss: 0.00012156076908373396\n",
            "Epoch 4900, Loss: 0.00011358620402303767\n",
            "Epoch 5000, Loss: 0.0001061956649270524\n",
            "Epoch 5100, Loss: 9.933471624702932e-05\n",
            "Epoch 5200, Loss: 9.295578139629635e-05\n",
            "Epoch 5300, Loss: 8.701713542329455e-05\n",
            "Epoch 5400, Loss: 8.148203795631653e-05\n",
            "Epoch 5500, Loss: 7.631799273244616e-05\n",
            "Epoch 5600, Loss: 7.149611918752569e-05\n",
            "Epoch 5700, Loss: 6.699062178352816e-05\n",
            "Epoch 5800, Loss: 6.277834357991976e-05\n",
            "Epoch 5900, Loss: 5.883839173132942e-05\n",
            "Epoch 6000, Loss: 5.5151823921479844e-05\n",
            "Epoch 6100, Loss: 5.17013860959664e-05\n",
            "Epoch 6200, Loss: 4.8471293155825824e-05\n",
            "Epoch 6300, Loss: 4.544704547488332e-05\n",
            "Epoch 6400, Loss: 4.261527518472404e-05\n",
            "Epoch 6500, Loss: 3.996361712487746e-05\n",
            "Epoch 6600, Loss: 3.748060018463525e-05\n",
            "Epoch 6700, Loss: 3.5155555474983235e-05\n",
            "Epoch 6800, Loss: 3.29785383749743e-05\n",
            "Epoch 6900, Loss: 3.094026200850088e-05\n",
            "Epoch 7000, Loss: 2.903204013680218e-05\n",
            "Epoch 7100, Loss: 2.7245737810463537e-05\n",
            "Epoch 7200, Loss: 2.5573728422535212e-05\n",
            "Epoch 7300, Loss: 2.4008856051058573e-05\n",
            "Epoch 7400, Loss: 2.2544402182809344e-05\n",
            "Epoch 7500, Loss: 2.1174056077539077e-05\n",
            "Epoch 7600, Loss: 1.989188816944076e-05\n",
            "Epoch 7700, Loss: 1.8692326015070144e-05\n",
            "Epoch 7800, Loss: 1.7570132388870784e-05\n",
            "Epoch 7900, Loss: 1.6520385202354983e-05\n",
            "Epoch 8000, Loss: 1.5538458983907904e-05\n",
            "Epoch 8100, Loss: 1.4620007705631178e-05\n",
            "Epoch 8200, Loss: 1.3760948783668366e-05\n",
            "Epoch 8300, Loss: 1.2957448110813367e-05\n",
            "Epoch 8400, Loss: 1.2205906006265036e-05\n",
            "Epoch 8500, Loss: 1.1502943988365205e-05\n",
            "Epoch 8600, Loss: 1.0845392292972265e-05\n",
            "Epoch 8700, Loss: 1.0230278073572127e-05\n",
            "Epoch 8800, Loss: 9.654814229961274e-06\n",
            "Epoch 8900, Loss: 9.116388820871147e-06\n",
            "Epoch 9000, Loss: 8.612555022676186e-06\n",
            "Epoch 9100, Loss: 8.141021601673777e-06\n",
            "Epoch 9200, Loss: 7.699643871667307e-06\n",
            "Epoch 9300, Loss: 7.286415111897619e-06\n",
            "Epoch 9400, Loss: 6.89945842299022e-06\n",
            "Epoch 9500, Loss: 6.53701900063921e-06\n",
            "Epoch 9600, Loss: 6.1974568083828915e-06\n",
            "Epoch 9700, Loss: 5.879239632088877e-06\n",
            "Epoch 9800, Loss: 5.580936499803576e-06\n",
            "Epoch 9900, Loss: 5.301211451433883e-06\n",
            "Test MSE: 3.222666869648232e-06\n",
            "Enter value for x1: 1\n",
            "Enter value for x2: 2\n",
            "Predicted output for x1=1.0, x2=2.0: 0.41800859747231034\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "def mse(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred) ** 2)\n",
        "class ANN:\n",
        "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weights_input_hidden = np.random.rand(self.input_size, self.hidden_size)\n",
        "        self.weights_hidden_output = np.random.rand(self.hidden_size, self.output_size)\n",
        "        self.bias_hidden = np.random.rand(self.hidden_size)\n",
        "        self.bias_output = np.random.rand(self.output_size)\n",
        "    def forward(self, X):\n",
        "        self.hidden_input = np.dot(X, self.weights_input_hidden) + self.bias_hidden\n",
        "        self.hidden_output = sigmoid(self.hidden_input)\n",
        "        self.final_input = np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output\n",
        "        self.final_output = self.final_input\n",
        "        return self.final_output\n",
        "    def backward(self, X, y, output):\n",
        "        output_error = y - output\n",
        "        output_delta = output_error\n",
        "        hidden_error = output_delta.dot(self.weights_hidden_output.T)\n",
        "        hidden_delta = hidden_error * sigmoid_derivative(self.hidden_output)\n",
        "        self.weights_input_hidden += X.T.dot(hidden_delta) * self.learning_rate\n",
        "        self.weights_hidden_output += self.hidden_output.T.dot(output_delta) * self.learning_rate\n",
        "        self.bias_hidden += np.sum(hidden_delta, axis=0) * self.learning_rate\n",
        "        self.bias_output += np.sum(output_delta, axis=0) * self.learning_rate\n",
        "    def train(self, X, y, epochs):\n",
        "        for epoch in range(epochs):\n",
        "            output = self.forward(X)\n",
        "            self.backward(X, y, output)\n",
        "            if epoch % 100 == 0:\n",
        "                loss = mse(y, output)\n",
        "                print(f'Epoch {epoch}, Loss: {loss}')\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.forward(X)\n",
        "X_train = np.array([[1, 2], [2, 3], [5, 6], [6, 7], [7, 8], [8, 9]])\n",
        "y_train = np.array([[0.4140], [0.4611], [0.5501], [0.5656], [0.5765], [0.5840]])\n",
        "X_test = np.array([[3, 4], [4, 5]])\n",
        "y_test = np.array([[0.4992], [0.5285]])\n",
        "input_size = 2\n",
        "hidden_size = 1\n",
        "output_size = 1\n",
        "learning_rate = 0.01\n",
        "epochs = 10000\n",
        "ann = ANN(input_size, hidden_size, output_size, learning_rate)\n",
        "ann.train(X_train, y_train, epochs)\n",
        "predictions = ann.predict(X_test)\n",
        "test_loss = mse(y_test, predictions)\n",
        "print(f'Test MSE: {test_loss}')\n",
        "x1 = float(input(\"Enter value for x1: \"))\n",
        "x2 = float(input(\"Enter value for x2: \"))\n",
        "user_input = np.array([[x1, x2]])\n",
        "prediction = ann.predict(user_input)\n",
        "print(f'Predicted output for x1={x1}, x2={x2}: {prediction[0][0]}')\n"
      ]
    }
  ]
}